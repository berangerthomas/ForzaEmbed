import re
import warnings
from collections import Counter

import matplotlib.pyplot as plt
import nltk
import numpy as np
import torch
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
from transformers import pipeline

warnings.filterwarnings("ignore")

# T√©l√©charger les ressources NLTK n√©cessaires
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")

try:
    nltk.data.find("corpora/stopwords")
except LookupError:
    nltk.download("stopwords")


class TextClusterAnalyzer:
    def __init__(
        self,
        model_name="distiluse-base-multilingual-cased",
        llm_model="microsoft/DialoGPT-small",
    ):
        """
        Initialise l'analyseur avec un mod√®le de sentence transformers et un petit LLM
        """
        print(f"Chargement du mod√®le d'embeddings {model_name}...")
        self.model = SentenceTransformer(model_name)

        # Initialiser un petit mod√®le de langage pour la g√©n√©ration de th√®mes
        print(f"Chargement du mod√®le de langage {llm_model}...")
        try:
            # Utiliser un mod√®le plus petit et efficace pour la g√©n√©ration
            self.theme_generator = pipeline(
                "text-generation",
                model="distilgpt2",  # Mod√®le l√©ger et rapide
                tokenizer="distilgpt2",
                device=0 if torch.cuda.is_available() else -1,
                max_length=50,
                do_sample=True,
                temperature=0.7,
                pad_token_id=50256,
            )
            self.llm_available = True
            print("‚úÖ Mod√®le de langage charg√© avec succ√®s")
        except Exception as e:
            print(f"‚ö†Ô∏è Impossible de charger le mod√®le de langage: {e}")
            print("üîÑ Utilisation du mode de g√©n√©ration basique")
            self.llm_available = False
            self.theme_generator = None

        self.stop_words = set(stopwords.words("french") + stopwords.words("english"))
        self.sentences = []
        self.embeddings = None
        self.labels = None
        self.themes = {}

    def preprocess_text(self, text):
        """
        Pr√©traite le texte en le divisant en phrases
        """
        # Nettoyer le texte
        text = re.sub(r"\s+", " ", text)  # Normaliser les espaces
        text = text.strip()

        # Diviser en phrases
        sentences = sent_tokenize(text, language="french")

        # Filtrer les phrases trop courtes
        sentences = [s for s in sentences if len(s.split()) > 3]

        self.sentences = sentences
        print(f"Texte divis√© en {len(sentences)} phrases.")
        return sentences

    def generate_embeddings(self):
        """
        G√©n√®re les embeddings pour chaque phrase
        """
        print("G√©n√©ration des embeddings...")
        self.embeddings = self.model.encode(self.sentences)
        print(f"Embeddings g√©n√©r√©s: {self.embeddings.shape}")
        return self.embeddings

    def find_optimal_clusters(self, max_clusters=None, min_clusters=2):
        """
        Trouve automatiquement le nombre optimal de clusters
        """
        n_sentences = len(self.sentences)

        # Validation pr√©liminaire
        if n_sentences < 2:
            print("‚ö†Ô∏è Pas assez de phrases pour optimiser les clusters.")
            return 1

        # D√©terminer la plage de clusters √† tester
        if max_clusters is None:
            # R√®gle heuristique : maximum = racine carr√©e du nombre de phrases
            max_clusters = min(int(np.sqrt(n_sentences)), 10)

        # S'assurer que max_clusters ne d√©passe pas le nombre possible
        max_clusters = min(max_clusters, n_sentences - 1)
        min_clusters = max(min_clusters, 2)

        # Validation finale
        if max_clusters < min_clusters:
            print(
                f"‚ö†Ô∏è Pas assez de phrases ({n_sentences}) pour avoir {min_clusters} clusters minimum."
            )
            return max(1, n_sentences - 1)

        if max_clusters == min_clusters:
            print(
                f"üìù Utilisation de {min_clusters} clusters (nombre optimal par d√©faut)."
            )
            return min_clusters

        print(
            f"üîç Recherche du nombre optimal de clusters (de {min_clusters} √† {max_clusters})..."
        )

        # M√©thodes d'√©valuation
        silhouette_scores = []
        inertias = []
        cluster_range = range(min_clusters, max_clusters + 1)

        for n_clusters in cluster_range:
            try:
                # Effectuer le clustering
                kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                labels = kmeans.fit_predict(self.embeddings)

                # Calculer le score de silhouette
                silhouette_avg = silhouette_score(self.embeddings, labels)
                silhouette_scores.append(silhouette_avg)

                # Calculer l'inertie (pour la m√©thode du coude)
                inertias.append(kmeans.inertia_)

                print(f"  üìä {n_clusters} clusters: silhouette = {silhouette_avg:.3f}")

            except Exception as e:
                print(f"  ‚ùå Erreur avec {n_clusters} clusters: {e}")
                # Ajouter des valeurs par d√©faut pour maintenir la coh√©rence
                silhouette_scores.append(0.0)
                inertias.append(float("inf"))

        # V√©rifier qu'on a au moins un score valide
        if not silhouette_scores or all(score <= 0 for score in silhouette_scores):
            print(
                "‚ö†Ô∏è Impossible d'optimiser les clusters. Utilisation de la valeur par d√©faut."
            )
            return min(3, n_sentences - 1)

        # M√©thode 1: Meilleur score de silhouette
        best_silhouette_idx = np.argmax(silhouette_scores)
        optimal_silhouette = cluster_range[best_silhouette_idx]

        # M√©thode 2: M√©thode du coude (elbow method)
        optimal_elbow = self._find_elbow_point(list(cluster_range), inertias)

        # M√©thode 3: Analyse de la coh√©rence des clusters
        optimal_coherence = self._evaluate_cluster_coherence(
            cluster_range, silhouette_scores
        )

        # Choisir le meilleur compromis
        candidates = [optimal_silhouette, optimal_elbow, optimal_coherence]
        candidate_scores = []

        for candidate in candidates:
            if candidate and min_clusters <= candidate <= max_clusters:
                idx = candidate - min_clusters
                if 0 <= idx < len(silhouette_scores):
                    score = silhouette_scores[idx]
                    candidate_scores.append((candidate, score))

        # S√©lectionner le candidat avec le meilleur score de silhouette
        if candidate_scores:
            optimal_clusters = max(candidate_scores, key=lambda x: x[1])[0]
        else:
            optimal_clusters = optimal_silhouette

        print(f"‚úÖ Nombre optimal d√©tect√©: {optimal_clusters} clusters")
        print(
            f"   üéØ Silhouette: {optimal_silhouette} (score: {silhouette_scores[optimal_silhouette - min_clusters]:.3f})"
        )
        print(f"   üìê Elbow: {optimal_elbow}")
        print(f"   üß† Coh√©rence: {optimal_coherence}")

        return optimal_clusters

    def _find_elbow_point(self, cluster_range, inertias):
        """
        Trouve le point de coude dans la courbe d'inertie
        """
        if len(inertias) < 3:
            return None

        # Calculer les diff√©rences secondes pour trouver le coude
        diffs = np.diff(inertias)
        second_diffs = np.diff(diffs)

        # Le coude est o√π la seconde d√©riv√©e est maximale (changement le plus fort)
        if len(second_diffs) > 0:
            elbow_idx = (
                np.argmax(second_diffs) + 2
            )  # +2 car on a perdu 2 points avec les diff√©rences
            if elbow_idx < len(cluster_range):
                return cluster_range[elbow_idx]

        return None

    def _evaluate_cluster_coherence(self, cluster_range, silhouette_scores):
        """
        √âvalue la coh√©rence des clusters pour choisir un nombre optimal
        """
        if len(silhouette_scores) < 2:
            return None

        # Chercher un plateau dans les scores de silhouette
        # Un bon nombre de clusters a un score √©lev√© et stable

        best_score = max(silhouette_scores)
        threshold = best_score * 0.95  # 95% du meilleur score

        # Trouver tous les candidats au-dessus du seuil
        good_candidates = []
        for i, score in enumerate(silhouette_scores):
            if score >= threshold:
                good_candidates.append(cluster_range[i])

        if good_candidates:
            # Pr√©f√©rer un nombre mod√©r√© de clusters (ni trop peu, ni trop)
            mid_point = len(cluster_range) / 2
            distances = [abs(c - mid_point) for c in good_candidates]
            best_idx = np.argmin(distances)
            return good_candidates[best_idx]

        return None

    def cluster_sentences(self, n_clusters=None, auto_detect=True):
        """
        Regroupe les phrases en clusters avec d√©tection automatique du nombre optimal
        """
        n_sentences = len(self.sentences)

        # Validation du nombre de phrases
        if n_sentences < 2:
            print(
                "‚ö†Ô∏è Pas assez de phrases pour effectuer un clustering (minimum 2 requises)."
            )
            # Cr√©er un cluster unique
            self.labels = np.zeros(n_sentences, dtype=int)
            return self.labels

        if auto_detect and n_clusters is None:
            # D√©tection automatique du nombre optimal de clusters
            n_clusters = self.find_optimal_clusters()
        elif n_clusters is None:
            # Valeur par d√©faut si pas de d√©tection automatique
            n_clusters = min(5, n_sentences - 1)  # Ne peut pas d√©passer n_sentences-1

        # Validation critique : s'assurer que n_clusters <= n_sentences
        if n_clusters >= n_sentences:
            print(
                f"‚ö†Ô∏è Ajustement: {n_clusters} clusters demand√©s mais seulement {n_sentences} phrases disponibles."
            )
            n_clusters = max(1, n_sentences - 1)  # Maximum possible
            if n_clusters == 1:
                print(
                    "üìù Cr√©ation d'un cluster unique (pas assez de phrases pour diviser)."
                )
                self.labels = np.zeros(n_sentences, dtype=int)
                return self.labels

        print(f"üéØ Clustering en {n_clusters} groupes...")

        try:
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            self.labels = kmeans.fit_predict(self.embeddings)

            # Calculer et afficher les m√©triques de qualit√©
            if len(set(self.labels)) > 1:  # V√©rifier qu'il y a plusieurs clusters
                silhouette_avg = silhouette_score(self.embeddings, self.labels)
                print(f"üìä Score de silhouette: {silhouette_avg:.3f}")

        except Exception as e:
            print(f"‚ùå Erreur lors du clustering: {e}")
            print("üîÑ Cr√©ation d'un cluster unique en fallback.")
            self.labels = np.zeros(n_sentences, dtype=int)
            return self.labels

        # Afficher la distribution des clusters
        cluster_counts = Counter(self.labels)
        print("üìà Distribution des clusters:")
        for cluster_id, count in sorted(cluster_counts.items()):
            percentage = (count / len(self.sentences)) * 100
            print(f"  Cluster {cluster_id}: {count} phrases ({percentage:.1f}%)")

        return self.labels

    def extract_cluster_themes(self):
        """
        Analyse chaque cluster pour proposer un th√®me
        """
        print("Extraction des th√®mes par cluster...")

        for cluster_id in np.unique(self.labels):
            # R√©cup√©rer les phrases du cluster
            cluster_sentences = [
                self.sentences[i]
                for i in range(len(self.sentences))
                if self.labels[i] == cluster_id
            ]

            # Combiner toutes les phrases du cluster
            cluster_text = " ".join(cluster_sentences)

            # Extraire les mots-cl√©s avec TF-IDF
            vectorizer = TfidfVectorizer(
                max_features=20,
                stop_words=list(self.stop_words),
                ngram_range=(1, 2),
                min_df=1,
            )

            try:
                # Comparer avec tous les autres textes pour avoir un bon TF-IDF
                all_texts = []
                for cid in np.unique(self.labels):
                    other_sentences = [
                        self.sentences[i]
                        for i in range(len(self.sentences))
                        if self.labels[i] == cid
                    ]
                    all_texts.append(" ".join(other_sentences))

                tfidf_matrix = vectorizer.fit_transform(all_texts)
                feature_names = vectorizer.get_feature_names_out()

                # R√©cup√©rer les scores TF-IDF pour ce cluster
                cluster_idx = list(np.unique(self.labels)).index(cluster_id)
                tfidf_scores = tfidf_matrix[cluster_idx].toarray()[0]

                # Obtenir les top mots-cl√©s
                top_indices = tfidf_scores.argsort()[-10:][::-1]
                keywords = [
                    feature_names[i] for i in top_indices if tfidf_scores[i] > 0
                ]

                # G√©n√©rer un th√®me bas√© sur les mots-cl√©s et l'analyse s√©mantique
                if keywords:
                    theme = self.generate_theme_from_keywords(
                        keywords, cluster_sentences
                    )
                else:
                    # M√™me sans mots-cl√©s TF-IDF, essayer d'extraire un th√®me des phrases
                    theme = self._extract_theme_from_sentences(cluster_sentences)

                self.themes[cluster_id] = {
                    "theme": theme,
                    "keywords": keywords[:5],
                    "sentences_count": len(cluster_sentences),
                    "sample_sentence": cluster_sentences[0]
                    if cluster_sentences
                    else "",
                }

            except Exception as e:
                print(f"Erreur lors de l'analyse du cluster {cluster_id}: {e}")
                # M√™me en cas d'erreur, essayer d'extraire un th√®me des phrases
                cluster_sentences = [
                    self.sentences[i]
                    for i in range(len(self.sentences))
                    if self.labels[i] == cluster_id
                ]
                theme = self._extract_theme_from_sentences(cluster_sentences)

                self.themes[cluster_id] = {
                    "theme": theme,
                    "keywords": [],
                    "sentences_count": len(cluster_sentences),
                    "sample_sentence": cluster_sentences[0]
                    if cluster_sentences
                    else "",
                }

        return self.themes

    def generate_theme_from_keywords(self, keywords, sentences):
        """
        G√©n√®re automatiquement un nom de th√®me en utilisant un mod√®le de langage
        """
        if not keywords:
            return "Th√®me G√©n√©ral"

        if self.llm_available:
            return self._generate_theme_with_llm(keywords, sentences)
        else:
            return self._generate_theme_fallback(keywords, sentences)

    def _generate_theme_with_llm(self, keywords, sentences):
        """
        Utilise un petit mod√®le de langage pour g√©n√©rer un th√®me naturel
        """
        try:
            # Pr√©parer le contexte pour le mod√®le
            context_keywords = ", ".join(keywords[:5])
            sample_sentence = sentences[0][:100] if sentences else ""

            # Cr√©er plusieurs prompts pour avoir de la vari√©t√©
            prompts = [
                f"Based on keywords: {context_keywords}. Theme:",
                f"Topic about: {context_keywords}. Main theme:",
                f"Subject: {context_keywords}. Category:",
            ]

            best_theme = None
            best_score = 0

            for prompt in prompts:
                try:
                    # G√©n√©rer avec le mod√®le
                    generated = self.theme_generator(
                        prompt,
                        max_length=len(prompt.split()) + 8,  # Limiter la g√©n√©ration
                        num_return_sequences=1,
                        temperature=0.8,
                        do_sample=True,
                        pad_token_id=50256,
                    )

                    # Extraire le th√®me g√©n√©r√©
                    full_text = generated[0]["generated_text"]
                    theme_part = full_text.replace(prompt, "").strip()

                    # Nettoyer le th√®me g√©n√©r√©
                    theme = self._clean_generated_theme(theme_part, keywords)

                    # √âvaluer la qualit√© du th√®me g√©n√©r√©
                    score = self._evaluate_theme_quality(theme, keywords)

                    if score > best_score:
                        best_score = score
                        best_theme = theme

                except Exception:
                    continue

            # Si on a un bon th√®me g√©n√©r√©, l'utiliser
            if best_theme and best_score > 0.3:
                return best_theme

        except Exception as e:
            print(f"Erreur lors de la g√©n√©ration avec LLM: {e}")

        # Fallback vers la m√©thode alternative
        return self._generate_theme_fallback(keywords, sentences)

    def _clean_generated_theme(self, raw_theme, keywords):
        """
        Nettoie et am√©liore le th√®me g√©n√©r√© par le LLM
        """
        if not raw_theme:
            return None

        # Supprimer les caract√®res ind√©sirables
        theme = re.sub(r"[^\w\s&-]", "", raw_theme)
        theme = re.sub(r"\s+", " ", theme).strip()

        # Prendre seulement les premiers mots (√©viter les phrases trop longues)
        words = theme.split()[:4]  # Maximum 4 mots
        theme = " ".join(words)

        # Capitaliser proprement
        if theme:
            # Capitaliser chaque mot important
            theme = " ".join(
                word.capitalize() if len(word) > 2 else word for word in theme.split()
            )

        return theme if theme and len(theme) > 2 else None

    def _evaluate_theme_quality(self, theme, keywords):
        """
        √âvalue la qualit√© d'un th√®me g√©n√©r√©
        """
        if not theme or len(theme) < 3:
            return 0

        score = 0
        theme_lower = theme.lower()

        # Points pour la pertinence avec les mots-cl√©s
        for keyword in keywords[:3]:
            if any(part.lower() in theme_lower for part in keyword.split()):
                score += 0.3

        # Points pour la longueur appropri√©e
        word_count = len(theme.split())
        if 1 <= word_count <= 4:
            score += 0.3

        # Points pour l'absence de mots parasites
        bad_words = [
            "the",
            "and",
            "or",
            "but",
            "in",
            "on",
            "at",
            "to",
            "for",
            "of",
            "with",
        ]
        if not any(bad in theme_lower for bad in bad_words):
            score += 0.2

        # Points pour la coh√©rence (pas de r√©p√©titions)
        words = theme.split()
        if len(set(word.lower() for word in words)) == len(words):
            score += 0.2

        return min(score, 1.0)

    def _generate_theme_fallback(self, keywords, sentences):
        """
        M√©thode de fallback pour g√©n√©rer des th√®mes sans LLM
        """
        # M√©thode bas√©e sur l'analyse s√©mantique intelligente
        primary_keywords = keywords[:3]

        # Analyser les patterns s√©mantiques
        theme_candidates = self._extract_semantic_themes(primary_keywords, sentences)

        # G√©n√©rer un th√®me intelligent
        return self._create_intelligent_theme(primary_keywords, theme_candidates)

    def _create_intelligent_theme(self, keywords, context_words):
        """
        Cr√©e un th√®me intelligent bas√© sur l'analyse s√©mantique
        """
        all_words = keywords + context_words

        # Essayer de cr√©er un th√®me conceptuel
        conceptual_theme = self._create_conceptual_theme(all_words)
        if conceptual_theme:
            return conceptual_theme

        # Sinon, combiner intelligemment les mots-cl√©s
        return self._combine_keywords_creatively(keywords)

    def _create_conceptual_theme(self, words):
        """
        Cr√©e un th√®me conceptuel bas√© sur l'analyse des mots
        """
        word_string = " ".join(words).lower()

        # Identifier des concepts abstraits
        concept_indicators = {
            "innovation": [
                "nouveau",
                "innovation",
                "technologie",
                "d√©veloppement",
                "avanc√©",
            ],
            "transformation": [
                "changement",
                "√©volution",
                "transformation",
                "mutation",
                "adaptation",
            ],
            "analyse": ["√©tude", "recherche", "analyse", "examen", "investigation"],
            "gestion": [
                "management",
                "gestion",
                "organisation",
                "administration",
                "contr√¥le",
            ],
            "d√©veloppement": [
                "croissance",
                "expansion",
                "progr√®s",
                "am√©lioration",
                "d√©veloppement",
            ],
            "communication": [
                "information",
                "communication",
                "message",
                "dialogue",
                "√©change",
            ],
            "strat√©gie": [
                "planification",
                "strat√©gie",
                "tactique",
                "approche",
                "m√©thode",
            ],
        }

        # Calculer les scores conceptuels
        concept_scores = {}
        for concept, indicators in concept_indicators.items():
            score = sum(1 for indicator in indicators if indicator in word_string)
            if score > 0:
                concept_scores[concept] = score

        # Combiner avec les mots principaux
        if concept_scores:
            best_concept = max(concept_scores.items(), key=lambda x: x[1])[0]
            main_word = words[0] if words else best_concept
            return f"{main_word.title()} & {best_concept.title()}"

        return None

    def _combine_keywords_creatively(self, keywords):
        """
        Combine cr√©ativement les mots-cl√©s pour un th√®me original
        """
        if not keywords:
            return "Th√®me √âmergent"

        # Nettoyer les mots-cl√©s
        clean_words = []
        for kw in keywords[:2]:
            words = re.findall(r"\b[a-zA-Z√Ä-√ø]+\b", kw)
            clean_words.extend([w for w in words if len(w) > 2])

        if not clean_words:
            return "Concept Principal"

        # Cr√©er des combinaisons cr√©atives
        if len(clean_words) == 1:
            return f"Domaine {clean_words[0].title()}"
        elif len(clean_words) >= 2:
            # Essayer diff√©rentes combinaisons
            combinations = [
                f"{clean_words[0].title()} & {clean_words[1].title()}",
                f"Enjeux {clean_words[0].title()}",
                f"Univers {clean_words[0].title()}",
                f"{clean_words[0].title()} Moderne",
            ]

            # Choisir la combinaison la plus √©quilibr√©e
            return combinations[0]  # Pr√©f√©rer la combinaison simple

        return "Th√©matique Sp√©cialis√©e"

    def _extract_semantic_themes(self, keywords, sentences):
        """
        Extrait des th√®mes potentiels en analysant le contexte s√©mantique
        """
        # Analyser les mots qui apparaissent fr√©quemment avec les mots-cl√©s
        context_words = []

        for sentence in sentences:
            words = word_tokenize(sentence.lower())
            words = [
                w
                for w in words
                if w.isalpha() and w not in self.stop_words and len(w) > 2
            ]

            # Chercher les mots qui apparaissent pr√®s des mots-cl√©s
            for keyword in keywords:
                if any(kw_part in sentence.lower() for kw_part in keyword.split()):
                    context_words.extend(words)

        # Compter les cooccurrences
        context_counter = Counter(context_words)

        # Extraire les mots de contexte les plus fr√©quents (excluant les mots-cl√©s originaux)
        filtered_context = []
        for word, count in context_counter.most_common(10):
            if not any(word in kw.lower() or kw.lower() in word for kw in keywords):
                filtered_context.append(word)

        return filtered_context[:5]

    def _generate_automatic_theme(self, primary_keywords, context_words):
        """
        G√©n√®re automatiquement un th√®me coh√©rent
        """
        all_theme_words = primary_keywords + context_words

        # Strat√©gie 1: Identifier des concepts centraux
        theme_name = self._identify_central_concept(all_theme_words)

        if theme_name:
            return theme_name

        # Strat√©gie 2: Combiner les mots-cl√©s principaux de mani√®re intelligente
        return self._combine_keywords_intelligently(primary_keywords)

    def _extract_theme_from_sentences(self, sentences):
        """
        Extrait un th√®me directement des phrases quand les mots-cl√©s TF-IDF ne suffisent pas
        """
        if not sentences:
            return "Th√®me G√©n√©ral"

        # Extraire tous les mots significatifs des phrases
        all_words = []
        for sentence in sentences:
            words = word_tokenize(sentence.lower())
            words = [
                w
                for w in words
                if w.isalpha() and w not in self.stop_words and len(w) > 3
            ]
            all_words.extend(words)

        # Compter les fr√©quences
        word_freq = Counter(all_words)
        top_words = [word for word, freq in word_freq.most_common(5)]

        # Utiliser la m√™me logique de g√©n√©ration automatique
        if top_words:
            return self._create_intelligent_theme(top_words, [])

        return "Th√®me G√©n√©ral"

    def _identify_central_concept(self, words):
        """
        Identifie un concept central √† partir des mots disponibles
        """
        # Analyser les patterns s√©mantiques automatiquement
        word_string = " ".join(words).lower()

        # D√©tection de domaines par analyse de patterns (extensible)
        domain_patterns = {
            # Patterns √©conomiques
            "√©conomie": [
                "march√©",
                "finance",
                "entreprise",
                "investissement",
                "commercial",
                "business",
                "√©conomique",
                "financier",
                "prix",
                "co√ªt",
                "profit",
                "industrie",
            ],
            # Patterns technologiques
            "technologie": [
                "digital",
                "num√©rique",
                "algorithme",
                "donn√©es",
                "intelligence",
                "artificielle",
                "technologique",
                "innovation",
                "syst√®me",
                "logiciel",
                "ordinateur",
            ],
            # Patterns de sant√©
            "sant√©": [
                "m√©dical",
                "patient",
                "traitement",
                "maladie",
                "diagnostic",
                "th√©rapie",
                "clinique",
                "h√¥pital",
                "m√©decin",
                "soins",
                "m√©dicament",
            ],
            # Patterns √©ducatifs
            "√©ducation": [
                "apprentissage",
                "√©tudiant",
                "formation",
                "enseignement",
                "p√©dagogique",
                "√©ducatif",
                "cours",
                "√©cole",
                "universit√©",
                "connaissance",
            ],
            # Patterns environnementaux
            "environnement": [
                "climatique",
                "√©cologique",
                "nature",
                "environnemental",
                "durable",
                "climat",
                "√©cologie",
                "plan√®te",
                "biodiversit√©",
                "√©nergie",
            ],
            # Patterns sociaux
            "soci√©t√©": [
                "social",
                "communaut√©",
                "population",
                "citoyen",
                "public",
                "collectif",
                "humain",
                "culturel",
                "soci√©tal",
                "groupe",
            ],
            # Patterns politiques
            "politique": [
                "gouvernement",
                "politique",
                "√©tat",
                "public",
                "administration",
                "pouvoir",
                "autorit√©",
                "institution",
                "r√©gulation",
            ],
        }

        # Calculer les scores de correspondance
        domain_scores = {}
        for domain, patterns in domain_patterns.items():
            score = sum(1 for pattern in patterns if pattern in word_string)
            if score > 0:
                domain_scores[domain] = score

        # Retourner le domaine avec le meilleur score
        if domain_scores:
            best_domain = max(domain_scores.items(), key=lambda x: x[1])[0]
            return best_domain.title()

        return None

    def _combine_keywords_intelligently(self, keywords):
        """
        Combine intelligemment les mots-cl√©s pour cr√©er un th√®me coh√©rent
        """
        if not keywords:
            return "Th√®me G√©n√©ral"

        # Nettoyer et pr√©parer les mots-cl√©s
        clean_keywords = []
        for kw in keywords[:2]:  # Prendre les 2 premiers mots-cl√©s
            # Enlever les caract√®res sp√©ciaux et diviser les mots compos√©s
            clean_words = re.findall(r"\b[a-zA-Z√Ä-√ø]+\b", kw)
            clean_keywords.extend(clean_words)

        # Supprimer les doublons tout en pr√©servant l'ordre
        seen = set()
        unique_keywords = []
        for kw in clean_keywords:
            if kw.lower() not in seen and len(kw) > 2:
                seen.add(kw.lower())
                unique_keywords.append(kw)

        if not unique_keywords:
            return "Th√®me G√©n√©ral"

        # Strat√©gies de combinaison
        if len(unique_keywords) == 1:
            return unique_keywords[0].title()
        elif len(unique_keywords) == 2:
            return f"{unique_keywords[0].title()} & {unique_keywords[1].title()}"
        else:
            # Pour 3+ mots, prendre les 2 plus significatifs
            main_concept = unique_keywords[0].title()
            secondary = unique_keywords[1].title()
            return f"{main_concept} & {secondary}"

    def visualize_clusters(self, figsize=(12, 8)):
        """
        Visualise les clusters avec t-SNE
        """
        print("G√©n√©ration de la visualisation t-SNE...")

        # R√©duire la dimensionnalit√© avec t-SNE
        tsne = TSNE(
            n_components=2, random_state=42, perplexity=min(30, len(self.sentences) - 1)
        )
        embeddings_2d = tsne.fit_transform(self.embeddings)

        # Cr√©er le graphique
        plt.figure(figsize=figsize)

        # Couleurs pour chaque cluster
        colors = plt.cm.Set3(np.linspace(0, 1, len(np.unique(self.labels))))

        # Tracer chaque cluster
        for cluster_id in np.unique(self.labels):
            mask = self.labels == cluster_id
            theme_name = self.themes[cluster_id]["theme"]

            plt.scatter(
                embeddings_2d[mask, 0],
                embeddings_2d[mask, 1],
                c=[colors[cluster_id]],
                label=f"Cluster {cluster_id}: {theme_name}",
                alpha=0.7,
                s=50,
            )

            # Ajouter le label du th√®me au centre du cluster
            cluster_center = embeddings_2d[mask].mean(axis=0)
            plt.annotate(
                theme_name,
                cluster_center,
                xytext=(5, 5),
                textcoords="offset points",
                bbox=dict(
                    boxstyle="round,pad=0.3", facecolor=colors[cluster_id], alpha=0.7
                ),
                fontsize=9,
                ha="center",
            )

        plt.title("Visualisation t-SNE des Clusters de Phrases", fontsize=16, pad=20)
        plt.xlabel("Dimension t-SNE 1")
        plt.ylabel("Dimension t-SNE 2")
        plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left")
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

    def print_analysis_summary(self):
        """
        Affiche un r√©sum√© de l'analyse
        """
        print("\n" + "=" * 50)
        print("R√âSUM√â DE L'ANALYSE")
        print("=" * 50)

        for cluster_id, info in self.themes.items():
            print(f"\nüéØ CLUSTER {cluster_id}: {info['theme']}")
            print(f"   üìä Nombre de phrases: {info['sentences_count']}")
            print(f"   üîë Mots-cl√©s: {', '.join(info['keywords'])}")
            print(f"   üìù Exemple: {info['sample_sentence'][:100]}...")

    def analyze(self, text, n_clusters=None, auto_detect=True):
        """
        Lance l'analyse compl√®te avec d√©tection automatique du nombre de clusters
        """
        print("üöÄ D√©but de l'analyse...")

        # √âtapes de l'analyse
        self.preprocess_text(text)
        self.generate_embeddings()

        # Clustering avec d√©tection automatique
        if auto_detect:
            print("üîç Mode d√©tection automatique activ√©")

        self.cluster_sentences(n_clusters, auto_detect)
        self.extract_cluster_themes()

        # Affichage des r√©sultats
        self.print_analysis_summary()
        self.visualize_clusters()

        return self.themes


# Exemple d'utilisation
if __name__ == "__main__":
    # Texte d'exemple
    sample_text = """
    L'intelligence artificielle transforme notre soci√©t√© de mani√®re profonde. Les algorithmes d'apprentissage automatique 
    permettent aux machines de comprendre et d'analyser des donn√©es complexes. Cette r√©volution technologique impacte 
    de nombreux secteurs d'activit√©.
    
    En √©conomie, les march√©s financiers utilisent des algorithmes de trading haute fr√©quence. Les entreprises investissent 
    massivement dans la transformation num√©rique. Les cryptomonnaies bouleversent les syst√®mes de paiement traditionnels. 
    L'analyse pr√©dictive aide les entreprises √† optimiser leurs strat√©gies.
    
    Dans le domaine de la sant√©, l'IA permet de diagnostiquer des maladies plus rapidement. Les m√©decins utilisent des 
    outils d'aide au diagnostic bas√©s sur l'apprentissage automatique. La t√©l√©m√©decine se d√©veloppe gr√¢ce aux nouvelles 
    technologies. Les patients b√©n√©ficient de traitements personnalis√©s.
    
    L'√©ducation conna√Æt √©galement une transformation majeure. Les plateformes d'apprentissage en ligne se multiplient. 
    Les √©tudiants acc√®dent √† des cours personnalis√©s gr√¢ce √† l'IA. Les enseignants utilisent des outils num√©riques 
    pour am√©liorer leurs m√©thodes p√©dagogiques.
    
    Cependant, ces changements soul√®vent des questions √©thiques importantes. La protection des donn√©es personnelles 
    devient cruciale. Les biais algorithmiques peuvent cr√©er des discriminations. Il faut r√©guler l'usage de ces 
    technologies pour prot√©ger les citoyens.
    
    Le changement climatique repr√©sente un d√©fi majeur pour l'humanit√©. Les √©missions de gaz √† effet de serre continuent 
    d'augmenter malgr√© les accords internationaux. Les √©nergies renouvelables se d√©veloppent mais pas assez rapidement. 
    Les gouvernements doivent acc√©l√©rer la transition √©cologique.
    """

    sample_text = """Biblioth√®que - Curis -au -Mont -d'Or

 Accueil
 Le village

 Pr√©sentation
 Histoire
 Patrimoine
 Phototh√®que

 Curis -au -Mont -d‚ÄôOr hier
 Curis -au -Mont -d‚ÄôOr aujourd‚Äôhui
 Curis -au -Mont -d‚ÄôOr - Photos des √©v√®nements

 Vie √©conomique
 Projets

 Projet r√©alis√©s Terrain des poiriers
 APPEL A PROJETS - BAR/RESTAURANT
 Projet r√©alis√© Engazonnement Cimeti√®re
 Projet r√©alis√© micro -cr√®che

 Plans

 Mairie

 Les Elus
 Le personnel communal
 Finances
 Urbanisme
 Equipements municipaux
 Commissions/syndicats
 Comptes -rendus des conseils municipaux
 Les arr√™t√©s municipaux temporaires
 Les arr√™t√©s municipaux permanents
 Le T√™tu
 Le Tambour

 Enfance et Social

 L‚Äô√©cole publique

 L‚Äô√©cole
 Le restaurant scolaire
 Le p√©riscolaire
 Portail Famille : inscriptions

 Relais d‚Äôassistantes maternelles
 Les centres de loisirs
 CCAS (Centre communal d‚Äôaction sociale)
 AIAD -Aide √† domicile
 Service de repas √† domicile
 Micro cr√®che

 Animation et Culture

 Biblioth√®que
 Associations

 Association communale de chasse
 Comit√© des f√™tes de Curis
 Iaido
 Body Karat√© -Karat√© D√©fense
 Sou des √©coles
 Sports et Loisirs
 ThouAMAPorte
 A Thouboutdechamps (jardin partag√©)
 De Thou Choeur

 Balades
 Planning des activit√©s
 Les √©v√©nements r√©currents √† Curis

 Cadre de Vie

 Environnement
 M√©tropole
 Collecte et traitement des d√©chets - gros appareils m√©nagers
 Comment obtenir une poubelle ?
 Vivre ensemble

 Propret√© canine
 Plantations (haies, arbres, arbustes‚Ä¶)
 Nuisances sonores / bruits de voisinage
 D√©neigement
 Br√ªlage des d√©chets verts
 Chiens en divagation
 Chenille processionnaire du pin

 Syndicat Mixte Plaines Monts d‚ÄôOr
 Plan Climat Communal
 Ambroisie

 Infos pratiques

 Formalit√©s administratives
 Le recensement militaire
 Num√©ros d‚Äôurgence
 Demandes et r√©clamations communautaires
 Infos transport
 Sant√© √† Curis
 Mutuelle Sant√©
 R√©server la salle du Vallon
 Le cimeti√®re
 Pr√™t de mobilier ext√©rieur
 Correspondant Progr√®s √† Curis
 Annoncer un √©v√©nement

 Contact

 Accueil " Animation et Culture " Biblioth√®que

 Acquisitions 2017 Beaton M La quiche fatale
 Bonnefoy M Sucre noir
 Bourdin F L'homme de leur vie
 Cayre H La daronne
 Chalandon Sorj Le jour d'avant
 Chantraine O Un √©l√©ment perturbateur
 Chavassieux C La vie vol√©e de Martin
 Cognetti P Les huit montagnes
 De Giovanni M Le noel du commissaire Ricciardi
 Deghelt F Agatha
 Deliry J La maraude
 Deserable F -H Un certain Monsieur Piekielny
 Djian P Marlene
 Ducrozet P L'invention des corps
 Dupuis M -B Les amants du presbytere
 Ellory R J Un c≈ìur sombre
 Ferrante E L'amie prodigieuse (3tomes)
 Gavalda A Fendre l'armure
 Giesbert F O Belle d'amour
 Giraud B Un loup pour l'homme
 Guez O La disparition de Josef Mengele
 Haenel Y Tiens ferme ta couronne
 Jaenada P La serpe
 Kemeid O Tangvald
 Khadra Y Ce que le mirage doit √† l'oasis
 Larsson A En sacrifice √† Moloch
 Ledig A De tes nouvelles
 Lenglet A Temps de haine
 Leon D Minuit sur le canal San Boldo
 Malte M Le gar√ßon
 Montero Manglano L La table du roi Salomon
 Nguyen V T Le sympathisant
 Nohant G L√©gende d'un dormeur √©veill√©
 Olafsdottir A A Or
 Pamuk O Cette chose √©trange en moi
 Pennac D Le cas Malaussene
 Recondo L Point cardinal
 Rufin J C Le tour du monde du roi Zibeline
 Sabolo M Summer
 Schwarz -Bart S Adieu Bogota
 Signol C La vie en son royaume
 Suter M Elephant
 Van Cauwelaert D Le retour de Jules
 Vargas F Quand sort la recluse
 Viel T Article 353 du code penal
 Wideman J -E Ecrire pour sauver une vie
 Yun M Les ames des enfants endormis
 Zeniter A L'art de perdre

 Biblioth√®que

 ACTUALITE
 La Biblioth√®que Municipale de Lyon (BML) est notre nouveau partenaire pour la fourniture d‚Äôun fond de 350 ouvrages disponible depuis le 15 mars 2018 .
 Retrouvez -les en t√©l√©chargeant la liste ici: Biblioth√®que municipale de Lyon ou venez les emprunter les vendredis (16h00 -17h30 ) et samedis matins (10h -12h).
 Vous pouvez aussi consulter:
 la liste des ouvrages acquis pour le fonds propre de la biblioth√®que municipale de Curis

 Acquisitions 2016
 Acquisitions 2017
 Acquisitions 2018
 Acquisitions 2020
 Acquisitions

 Les listes, par genres des ouvrages disponibles √† la Biblioth√®que de Curis:
 Bandes dessin√©es Documentaires Fantasia Livres en gros caract√®res Romans
 romans policiers
 La biblioth√®que, comment √ßa marche?
 Pr√©sentation
 Objectif :

 Mettre √† la disposition du public adulte des romans, des documents, des biographies.

 Moyens :

 Un budget annuel de 1200 euros pour acqu√©rir des livres.
 Un d√©p√¥t de la Biblioth√®que Municipale de Lyon renouvel√© r√©guli√®rement.
 Un fonds propre d‚Äôun millier d‚Äôouvrages.

 Enjeux :

 Proposer r√©guli√®rement des romans contemporains et √©changer des avis sur les lectures.

 Acteurs :

 Un groupe de b√©n√©voles passionn√©s par la lecture.

 Infos pratiques
 Quand ?
Les permanences ont lieu tous les samedis matin de 10 heures √† midi et les vendredis (hors vacances scolaires) de 16h00 √† 17h30
 O√π ?
Dans l‚Äôaile gauche de la Mairie, c√¥t√© garderie, rue de la Mairie.
 Comment s‚Äôinscrire ?
Si vous disposez d‚Äôune adresse mail valide, il vous suffit lors de votre premi√®re visite, de remplir une fiche d‚Äôinscription. Si vous ne disposez pas d‚Äôune adresse mail veuillez apporter 5 enveloppes timbr√©es et √† votre adresse.
 Emprunts : mode d‚Äôemploi
 Vous pouvez emprunter autant de livres que vous le souhaitez (mais avec un maximum de deux nouveaut√©s) et pour une dur√©e de trois semaines.
Nous pouvons vous ‚Äúcommander‚Äù des titres pr√©cis, des auteurs, des th√®mes,‚Ä¶ aupr√®s de la BML.
 Notre partenaire, la BML (Biblioth√®que Municipale de Lyon) :
Vous pouvez consulter le catalogue sur le site de la BML et nous demander de r√©server les ouvrages qui vous int√©ressent. Une navette nous livre tous les mois les ouvrages disponibles.
 Comit√© de lecture :
 Un groupe de lecture ouvert √† tous et associant les biblioth√®ques d‚ÄôAlbigny, Poleymieux, Saint Germain, Couzon et Curis se r√©unit tous les deux mois et permet d‚Äô√©changer des avis sur un choix de livres.

 Mairie

 Adresse :
Rue de la Mairie
69250 Curis -au -Mont -d'Or

 Contact :
T√©l√©phone : 04 .78 .91 .24 .02
T√©l√©copie : 04 .78 .98 .28 .05
Courriel : mairie@curis.fr

 Facebook
 Mentions l√©gales
 Plan du site
 Cr√©dits
 Contact

 Nous utilisons des cookies pour vous garantir la meilleure exp√©rience sur notre site. Si vous continuez √† utiliser ce dernier, nous consid√©rerons que vous acceptez l'utilisation des cookies. Ok

 Accueil
 Le village

 Pr√©sentation
 Histoire
 Patrimoine
 Phototh√®que

 Curis -au -Mont -d‚ÄôOr hier
 Curis -au -Mont -d‚ÄôOr aujourd‚Äôhui
 Curis -au -Mont -d‚ÄôOr - Photos des √©v√®nements

 Vie √©conomique
 Projets

 Projet r√©alis√©s Terrain des poiriers
 APPEL A PROJETS - BAR/RESTAURANT
 Projet r√©alis√© Engazonnement Cimeti√®re
 Projet r√©alis√© micro -cr√®che

 Plans

 Mairie

 Les Elus
 Le personnel communal
 Finances
 Urbanisme
 Equipements municipaux
 Commissions/syndicats
 Comptes -rendus des conseils municipaux
 Les arr√™t√©s municipaux temporaires
 Les arr√™t√©s municipaux permanents
 Le T√™tu
 Le Tambour

 Enfance et Social

 L‚Äô√©cole publique

 L‚Äô√©cole
 Le restaurant scolaire
 Le p√©riscolaire
 Portail Famille : inscriptions

 Relais d‚Äôassistantes maternelles
 Les centres de loisirs
 CCAS (Centre communal d‚Äôaction sociale)
 AIAD -Aide √† domicile
 Service de repas √† domicile
 Micro cr√®che

 Animation et Culture

 Biblioth√®que
 Associations

 Association communale de chasse
 Comit√© des f√™tes de Curis
 Iaido
 Body Karat√© -Karat√© D√©fense
 Sou des √©coles
 Sports et Loisirs
 ThouAMAPorte
 A Thouboutdechamps (jardin partag√©)
 De Thou Choeur

 Balades
 Planning des activit√©s
 Les √©v√©nements r√©currents √† Curis

 Cadre de Vie

 Environnement
 M√©tropole
 Collecte et traitement des d√©chets - gros appareils m√©nagers
 Comment obtenir une poubelle ?
 Vivre ensemble

 Propret√© canine
 Plantations (haies, arbres, arbustes‚Ä¶)
 Nuisances sonores / bruits de voisinage
 D√©neigement
 Br√ªlage des d√©chets verts
 Chiens en divagation
 Chenille processionnaire du pin

 Syndicat Mixte Plaines Monts d‚ÄôOr
 Plan Climat Communal
 Ambroisie

 Infos pratiques

 Formalit√©s administratives
 Le recensement militaire
 Num√©ros d‚Äôurgence
 Demandes et r√©clamations communautaires
 Infos transport
 Sant√© √† Curis
 Mutuelle Sant√©
 R√©server la salle du Vallon
 Le cimeti√®re
 Pr√™t de mobilier ext√©rieur
 Correspondant Progr√®s √† Curis
 Annoncer un √©v√©nement

 Contact"""

    # Cr√©er et lancer l'analyseur avec g√©n√©ration LLM et d√©tection automatique
    print("ü§ñ Initialisation avec mod√®le de langage pour g√©n√©ration de th√®mes...")
    analyzer = TextClusterAnalyzer()

    # Analyse avec d√©tection automatique du nombre de clusters
    print("\nüîç M√âTHODE 1: D√©tection automatique du nombre de clusters")
    themes_auto = analyzer.analyze(sample_text, auto_detect=True)

    # Exemple avec nombre fixe pour comparaison
    print("\n\nüéØ M√âTHODE 2: Nombre de clusters fix√© √† 4 (pour comparaison)")
    analyzer2 = TextClusterAnalyzer()
    themes_fixed = analyzer2.analyze(sample_text, n_clusters=4, auto_detect=False)

    print("\n" + "=" * 60)
    print("üí° INFORMATIONS TECHNIQUES")
    print("=" * 60)
    print("üîπ Embeddings: sentence-transformers (analyse s√©mantique)")
    print("üîπ Clustering: K-means avec d√©tection automatique optimale")
    print("üîπ Optimisation: Score de silhouette + M√©thode du coude + Coh√©rence")
    print("üîπ G√©n√©ration de th√®mes: DistilGPT-2 (LLM l√©ger)")
    print("üîπ Visualisation: t-SNE (r√©duction dimensionnelle)")
    print("üîπ Fallback: Analyse s√©mantique intelligente")

    # Instructions d'installation
    print("\nüì¶ D√âPENDANCES REQUISES:")
    print(
        "pip install sentence-transformers scikit-learn matplotlib nltk transformers torch"
    )

    print("\nüéõÔ∏è UTILISATION:")
    print("# D√©tection automatique (recommand√©)")
    print("analyzer.analyze(text)")
    print("# Nombre fixe")
    print("analyzer.analyze(text, n_clusters=5, auto_detect=False)")
